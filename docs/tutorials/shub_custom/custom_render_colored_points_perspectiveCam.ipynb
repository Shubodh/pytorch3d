{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "476513ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "# Util function for loading point clouds|\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9f970a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Pointclouds\n",
    "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    PerspectiveCameras, \n",
    "    PointsRasterizationSettings,\n",
    "    PointsRenderer,\n",
    "    PulsarPointsRenderer,\n",
    "    PointsRasterizer,\n",
    "    AlphaCompositor,\n",
    "    NormWeightedCompositor\n",
    ")\n",
    "\n",
    "from pytorch3d.utils import cameras_from_opencv_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58b391a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "file_num = '024'\n",
    "base_path_simsub = '/home/saishubodh/rrc_projects_2021/graphVPR_project/'\n",
    "end_path = 'Hierarchical-Localization/graphVPR/ideas_SG/place-graphVPR/rand_json'\n",
    "json_dir = base_path_simsub + end_path\n",
    "p3p_num = 'p3p_{}'.format(file_num)\n",
    "p3p_dir = os.path.join(json_dir, p3p_num)\n",
    "p3p_files = os.listdir(p3p_dir)\n",
    "\n",
    "# TODO1: should change this to explicitly throw error if DUC_cutout not found.\n",
    "for i in p3p_files:\n",
    "    if 'DUC_cutout' in i:\n",
    "        break\n",
    "\n",
    "\n",
    "base_mat_path = \"/scratch/saishubodh/InLoc_dataset/database/cutouts/DUC1/\"\n",
    "jpg_file = os.path.join(base_mat_path + file_num, i)\n",
    "pc_file = jpg_file + '.mat'\n",
    "json_file = p3p_num + '.json'\n",
    "json_file = os.path.join(p3p_dir, json_file)\n",
    "\n",
    "# uncomment for sanity check\n",
    "# print(jpg_file, os.path.isfile(jpg_file))\n",
    "# print(json_file, os.path.isfile(json_file))\n",
    "# print(pc_file, os.path.isfile(pc_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78a70a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create point cloud\n",
    "# TODO: modify/add loop for multi scans for entire room\n",
    "xyz_file  = loadmat(Path(pc_file))[\"XYZcut\"]\n",
    "rgb_file = loadmat(Path(pc_file))[\"RGBcut\"]\n",
    "xyz_sp = (xyz_file.shape)\n",
    "with open(json_file,'r') as f:\n",
    "    json_data = json.load(f)\n",
    "xyz_file = (xyz_file.reshape((xyz_sp[0]*xyz_sp[1] ,3)))\n",
    "rgb_file = (rgb_file.reshape((xyz_sp[0]*xyz_sp[1] ,3)))\n",
    "verts = torch.Tensor(xyz_file).to(device)\n",
    "rgb = torch.Tensor(rgb_file).to(device)\n",
    "point_cloud = Pointclouds(points=[verts], features=[rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "984daf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_np = np.array(json_data['extrinsic'])\n",
    "K_np = np.array(json_data['intrinsic']['intrinsic_matrix'])\n",
    "RT_np = RT_np.reshape(4,4).T\n",
    "K = K_np.reshape(3,3).T\n",
    "# print(RT_np)\n",
    "# print(K)\n",
    "\n",
    "fx, fy = K[0,0], K[1,1] # focal length in x and y axes\n",
    "px, py = K[0,2], K[1,2] # principal points in x and y axes\n",
    "R, t =  RT_np[0:3,0:3], np.array([RT_np[0:3,3]]) #  rotation and translation matrix\n",
    "\n",
    "# # First, (X, Y, Z) = R @ p_world + t, where p_world is 3D coordinte under world system\n",
    "# # To go from a coordinate under view system (X, Y, Z) to screen space, the perspective camera mode should consider\n",
    "# # the following transformation and we can get coordinates in screen space in the range of [0, W-1] and [0, H-1]\n",
    "# x_screen = fx * X / Z + px\n",
    "# y_screen = fy * Y / Z + py\n",
    "\n",
    "# In PyTorch3D, we need to build the input first in order to define camera. Note that we consider batch size N = 1\n",
    "RR = torch.from_numpy(R).unsqueeze(0) # dim = (1, 3, 3)\n",
    "KK = torch.from_numpy(K).unsqueeze(0) # dim = (1, 3, 3)\n",
    "# following line transposes matrix\n",
    "# RR = torch.from_numpy(R).permute(1,0).unsqueeze(0) # dim = (1, 3, 3) \n",
    "tt = torch.from_numpy(t) # dim = (1, 3)\n",
    "f = torch.tensor((fx, fy), dtype=torch.float32).unsqueeze(0) # dim = (1, 2)\n",
    "p = torch.tensor((px, py), dtype=torch.float32).unsqueeze(0) # dim = (1, 2)\n",
    "img_size = (1600, 1200)\n",
    "img_size_t = torch.tensor(img_size).unsqueeze(0) # (width, height) of the image\n",
    "# img_size_t = torch.tensor(img_size, dtype=torch.float32).unsqueeze(0) # (width, height) of the image\n",
    "# KK.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f0311d42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerspectiveCameras()"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cameras_pytorch3d = cameras_from_opencv_projection(RR.float(), tt.float(), KK.float(), img_size_t.float())\n",
    "# above line was giving dtype errors, so made everything float..\n",
    "cameras_pytorch3d.to(device)\n",
    "# (img_size_t.float().dtype, RR.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b00081b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerspectiveCameras()"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we can define the Perspective Camera model. \n",
    "# NOTE: you should consider negative focal length as input!!!\n",
    "cameras = PerspectiveCameras(R=RR, T=tt, focal_length=-f, principal_point=p,\n",
    "    device=device,image_size=(img_size,))\n",
    "cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "08c26faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "till here\n",
      "024_pytorchsyn.png saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(cameras.K)\n",
    "# unsure about radius and points_per_pixel\n",
    "raster_settings = PointsRasterizationSettings(\n",
    "    image_size=[1200,1600], \n",
    "    radius = 0.3,\n",
    "    points_per_pixel = 20\n",
    ")\n",
    "# You can modify AlphaCompositor\n",
    "# to change background color\n",
    "rasterizer = PointsRasterizer(cameras=cameras_pytorch3d, raster_settings=raster_settings)\n",
    "renderer = PointsRenderer(\n",
    "    rasterizer=rasterizer,\n",
    "    compositor=AlphaCompositor()\n",
    ")\n",
    "print(\"till here\")\n",
    "images = renderer(point_cloud)\n",
    "#We need to convert \n",
    "images_np = images[0, ..., :3].cpu().numpy()\n",
    "images_np[images_np < 0] = 0\n",
    "images_np = images_np.astype(np.uint8)\n",
    "plt.imsave(file_num + '_pytorchsyn.png',images_np)\n",
    "print(f\"{file_num}_pytorchsyn.png saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b13d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
